{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification task metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n1.Data Gathering\\n2.Data Exploration\\n3.Data preprocessing\\n4.Model Building\\n5.Evaluation of models'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "1.Data Gathering\n",
    "2.Data Exploration\n",
    "3.Data preprocessing\n",
    "4.Model Building\n",
    "5.Evaluation of models'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n                        Confusion Matrix\\n                        \\n                        (Predicted)\\n                        \\n                        pos       neg\\n                \\n                pos     TP         FN\\n      (actual)          \\n                neg     FP         TN\\n                \\n                \\n                accuracy = TP+TN/TP+FN+FP+TN\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "                        Confusion Matrix\n",
    "                        \n",
    "                        (Predicted)\n",
    "                        \n",
    "                        pos       neg\n",
    "                \n",
    "                pos     TP         FN\n",
    "      (actual)          \n",
    "                neg     FP         TN\n",
    "                \n",
    "                \n",
    "                accuracy = TP+TN/TP+FN+FP+TN\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n>>we cant only use accuracy as \\nevaluation metric on an unbalanced data\\n\\nreason:\\nConsider a scenario in which the model has classified all the \\ndata points as ‘No’ then the accuracy of the model is 99%. \\nIn fact, the predictive power of the model is zero since it \\nis classifying all the points as ‘No’. Hence, accuracy\\nis not the right metric in case of class imbalance problems.\\n\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    ">>we cant only use accuracy as \n",
    "evaluation metric on an unbalanced data\n",
    "\n",
    "reason:\n",
    "Consider a scenario in which the model has classified all the \n",
    "data points as ‘No’ then the accuracy of the model is 99%. \n",
    "In fact, the predictive power of the model is zero since it \n",
    "is classifying all the points as ‘No’. Hence, accuracy\n",
    "is not the right metric in case of class imbalance problems.\n",
    "\n",
    "''' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTotal actual positive = TP+FN\\n\\nTotal actual negative = FP+TN\\n                        \\n> True positive rate = TP/TP+FN  if high then good model\\n\\nFalse Negative rate = FN/TP+FN  if low then good\\n\\n> True negative rate = TN/FP+TN   if high then good model\\n\\nFalse positve rate = FP/FP+TN   if low then good\\n\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#alternate to accracy\n",
    "\n",
    "'''\n",
    "Total actual positive = TP+FN\n",
    "\n",
    "Total actual negative = FP+TN\n",
    "                        \n",
    "> True positive rate = TP/TP+FN  if high then good model\n",
    "\n",
    "False Negative rate = FN/TP+FN  if low then good\n",
    "\n",
    "> True negative rate = TN/FP+TN   if high then good model\n",
    "\n",
    "False positve rate = FP/FP+TN   if low then good\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPrecision: \\n\\nPredictions actually positive/total positive predictions\\n= TP/TP+FP\\nused when we cannot afford to have FP\\navoiding FP is needed than encountring FN \\n\\n\\nRecall:\\nPredictions actually positive/total actual predictions\\n=TP/TP+FN\\nused when we cannot afford to have FP\\navoiding FN is prioritzed over FP\\n\\nhigh precision, low recall\\nhigh recall, low precision\\n(precision and recall are used in diverse cases, using one of them is depend on the use case)\\n\\nboth combined to get good model, F1 is max when precision=recall\\n\\nf1= 2*precision*recall/precision + recall\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Precision: \n",
    "\n",
    "Predictions actually positive/total positive predictions\n",
    "= TP/TP+FP\n",
    "used when we cannot afford to have FP\n",
    "avoiding FP is needed than encountring FN \n",
    "\n",
    "\n",
    "Recall:(TPR)\n",
    "Predictions actually positive/total actual predictions\n",
    "=TP/TP+FN\n",
    "used when we cannot afford to have FP\n",
    "avoiding FN is prioritzed over FP\n",
    "\n",
    "high precision, low recall\n",
    "high recall, low precision\n",
    "(precision and recall are used in diverse cases, using one of them is depend on the use case)\n",
    "\n",
    "both combined to get good model, F1 is max when precision=recall\n",
    "\n",
    "f1= 2*precision*recall/precision + recall\n",
    "\n",
    "recall/precision is 0 then f1 is 0\n",
    "\n",
    "\n",
    ">>How do we decide which metric to use for evaluating the model?\n",
    "\n",
    "In the case of cancer data modeling, recall is preferred over precision,\n",
    "since, recall minimizes the false negatives i.e. \n",
    "classifying the actual cancer patient as cancer. \n",
    "Hence, choosing the right metric depends on the business problem.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nEvaluation metric for binary classification\\n\\nAUC-ROC(Area Under the  ROC Curve)\\nROC- Receiver Operating characteristic(distinguish noise from not noise)\\n\\ngives the tradeoff bw TP as=nd FP\\n\\nthe more the are under the curve the better is the model\\n\\nconsiders the order of probabilities not their values, we cant compare two models\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Evaluation metric for binary classification\n",
    "\n",
    "AUC-ROC(Area Under the  ROC Curve)\n",
    "ROC- Receiver Operating characteristic(distinguish noise from not noise)\n",
    "\n",
    "gives the tradeoff bw TP as=nd FP\n",
    "\n",
    "the more the are under the curve the better is the model\n",
    "\n",
    "considers the order of probabilities not their values, we cant compare two models\n",
    "\n",
    "AUC-ROC is a plot between the False Positive Rate(FPR) and True Positive Rate(TPR)\n",
    "\n",
    "The AUC ROC metric is preferred for skewed distributions \n",
    "since it takes into account both the TPR and FPR.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nLog loss:\\n    it is the negative avg of the log of corrected predicted proab \\n    for each instance\\n    \\n    =  {- /N sum of all log(pi)}\\n    \\n    = 1/N(summation from 1 to N( -( y*log(p) + (1-y) * log(1-p) )))\\n\\n        y is actual class either 1/0\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Log loss:\n",
    "    it is the negative avg of the log of corrected predicted proab \n",
    "    for each instance\n",
    "    \n",
    "    =  {- /N sum of all log(pi)}\n",
    "    \n",
    "    = 1/N(summation from 1 to N( -( y*log(p) + (1-y) * log(1-p) )))\n",
    "\n",
    "        y is actual class either 1/0\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression task metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Mean Absolute error = 1/n sumation of |predicted - actual|\n",
    "\n",
    "Mean squared error =  1/n sumation of (predicted - actual)^2 \n",
    "\n",
    ">Root mean square error = under root( 1/n sumation of (predicted - actual)^2 )\n",
    "\n",
    "Root mean squared log error = under root( 1/n sumation of ( log(predicted + 1) - log(actual+1) )^2)\n",
    "\n",
    "\n",
    "                           sumation of (predicted - actual)^2     MSE(model) =0.8\n",
    "Relative Squared error  = ------------------------------------- = ------------\n",
    "                           sumation of (mean - actual)^2          MSE(baseline)=0.5 (values just considered)\n",
    "            \n",
    "    \n",
    "if MSE(model)==MSE(baseline) then RSE = 1 : good predicting avg values\n",
    "                                          > 1 : worst than baseline value\n",
    "                                          \n",
    "lower the MSE value the better our model is \n",
    "\n",
    "\n",
    "R-Squared = 1- Relative Squared error\n",
    "more feaures ->R-Squared either inc or doesnt change, never dec\n",
    "\n",
    "adjusted R-Squared = 1- (1- R^2)[ (n-1)/(n-(k+1))]\n",
    "n: no of samples\n",
    "k: no of features\n",
    "\n",
    "            \n",
    "            \n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
